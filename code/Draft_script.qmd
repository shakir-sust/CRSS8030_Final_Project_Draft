---
title: "Data Science Applied to Ag - Final Project - ML"
format:
  html:
    embed-resources: true
    toc: true
    theme: cerulean
author: Md Shakir Moazzem, Umar Munir
---

# Start of data wrangling

# Introduction  

This script contains the data wrangling steps for the final project.  

# Setup  

##Loading packages  

The following code chunk will load necessary packages.

```{r Setup, message=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("readxl")
#install.packages("janitor")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("readr")
#install.packages("lubridata")
#install.packages("stringr")

# Loading packages 

library(tidyverse)
library(readxl) # to read excel files
library(janitor) # to clean data; helps fix and standardize the column names
library(dplyr) # wrangling
library(tidyr) # wrangling
library(readr) # to export csv
library(lubridate)
library(stringr)

```


## Reading data  

The following code chunk will read the csv files for the 3 training data sets

```{r training data import, message=F, warning=F}

#reading the csv files for the 3 training data sets 

trait_df <- read_csv("../data/training/training_trait.csv") 
meta_df  <- read_csv("../data/training/training_meta.csv")
soil_df  <- read_csv("../data/training/training_soil.csv")

```

## EDA

```{r}

summary(trait_df)

```
```{r}

summary(meta_df)

```



```{r}

summary(soil_df)

```


# Data wrangling on "trait_df"

The code below creates a function to adjust the yield_mg_ha for 15.5% grain moisture.

```{r}

# Function to transform yield to 15.5% moisture
adjust_yield <- function(yield_mg_ha, grain_moisture) {
  yield_mg_ha * (100 - grain_moisture) / (100 - 15.5)
}

```


The code below conducts data wrangling on "trait_df"

```{r clean_and_summarize_trait, message=F, warning=F}



trait_clean <- trait_df %>%
  select(-block) %>%
  mutate(
    site = str_remove_all(site, "[a-z]"), # removing all lowercase letters from site
    site = str_replace(site, "-.*$", ""), # keeping only text before any dash
    site = str_replace(site, "_.*$", "") # keeping only text before any underscore
  ) %>%
  group_by(year, site, hybrid) %>%
  summarize(
    yield_mg_ha    = mean(yield_mg_ha, na.rm = TRUE),
    grain_moisture = mean(grain_moisture, na.rm = TRUE),
    date_planted   = first(date_planted),
    date_harvested = first(date_harvested),
    .groups = "drop"
  ) %>%
  mutate(
    adjusted_yield = adjust_yield(yield_mg_ha, grain_moisture),
    planting_date  = as.Date(date_planted,   "%m/%j/%y"),
    harvest_date   = as.Date(date_harvested, "%m/%j/%y"),
    plant_doy      = yday(planting_date),
    harvest_doy    = yday(harvest_date),
  ) %>%
  select(-yield_mg_ha, -grain_moisture, -date_planted, -date_harvested, -planting_date, -harvest_date) %>%
  ungroup()

trait_avg_df


```



```{r exporting wrangled data set}

#write_csv(trait_avg_df,
          #"../data/training_trait_clean.csv")
```


# Data wrangling on "soil_df"

The following code chunk will conduct data wrangling on  "training_soil.csv"


```{r data_wrangling_training_soil, message=F, warning=F}

library(stringr)

soil_clean <- soil_df %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  ungroup()

summary(soil_clean)
```


```{r exporting wrangled data set}

#write_csv(soil_clean,
          #"../data/training_soil_clean.csv")
```


The following code chunk will conduct data wrangling on  "training_meta.csv".

```{r clean_meta_sites}

meta_clean <- meta_df %>%
  rename(
    lon = longitude,
    lat = latitude
  ) %>%
  mutate(
    # 1) remove all lowercase letters
    site = str_remove_all(site, "[a-z]"),
    # 2) keep only text before any dash
    site = str_replace(site, "-.*$", ""),
    # 3) keep only text before any underscore
    site = str_replace(site, "_.*$", "")
  ) %>%
  distinct(year, site, .keep_all = TRUE) %>%
  ungroup()

```


The code below will create categories for previous crops.


```{r}
library(stringr)

meta_clean <- meta_clean %>%
  mutate(
    # 1) lowercase & trim
    prev_crop_raw = str_to_lower(str_squish(previous_crop)),
    # 2) recode into your desired levels
    prev_crop = case_when(
      prev_crop_raw == "cotton"                                   ~ "cotton",
      prev_crop_raw == "peanut"                                   ~ "peanut",
      prev_crop_raw %in% c("soybean", "soybeans")                 ~ "soybean",
      prev_crop_raw == "sorghum"                                  ~ "sorghum",
      prev_crop_raw %in% c("wheat", "winter wheat", "2019/20 wheat") ~ "wheat",
      TRUE                                                         ~ "others"
    ) %>%
    # 3) make it a factor with the exact ordering you want
    factor(levels = c("cotton", "peanut", "soybean", "sorghum", "wheat", "others"))
  ) %>%
  select(-previous_crop, -prev_crop_raw) %>%
  clean_names()

meta_clean

```



```{r}
summary(meta_clean)
```


```{r exporting wrangled data set}

#write_csv(meta_clean,
          #"../data/training_meta_clean.csv")
```



```{r}

merged_clean  <- trait_avg_df %>%
  left_join(soil_clean, by = c("year", "site")) %>%
  left_join(meta_clean, by = c("year", "site"))

summary(merged_clean)
```

# Exporting  

The following code chunk will export the wrangled data into a .csv.

```{r exporting wrangled data set}

#write_csv(merged_clean,
          #"../data/training_merged_clean.csv")
```


# End of Data Wrangling 

#Start of Open Source Daymet weather data download

The following code chunk will load necessary packages.  

```{r Setup, message=F, warning=F}

# Installing packages

#install.packages("tidyverse")
#install.packages("sf") #to manipulate vector geospatial files
#install.packages("daymetr") #to retrieve data from daymet database website through R
#install.packages("remotes") #to install R packages that are not available on CRAN, and available on GitHub
#remotes::install_github("ropensci/USAboundaries") 
#remotes::install_github("ropensci/USAboundariesData")

# Loading packages

library(tidyverse) #need to load "tidyverse" package at first
library(sf) # for US map #to manipulate vector geo-spatial points
library(daymetr) #to retrieve data from daymet database website through R
library(remotes)
library(USAboundaries) # for US state boundaries
library(USAboundariesData)

```


```{r create map of USA and add points, message=F, warning=F}

states <- us_states() %>% 
  filter( !(state_abbr %in% c("PR", "AK", "HI")) ) #to remove "PR" (Puerto Rico), "AK" (Alaska), and "HI" (Hawaii) from the rows of "states" object i.e., to keep all states that are NOT "PR", "AK", and "HI" #we use "%in%" to filter (i.e., work on) more than one state/entry #if we wanted to filter just 1 state, we would use == sign e.g., !(state_abbr == c("PR"))
  
ggplot() +
  geom_sf(data = states) + #"geom_sf()" is used to plot "sf" object, which we just created above as "states" object; plots all states and territories of USA
  geom_point(data = merged_clean,
             aes(x = lon, #"Longitude" goes on longitude
                 y = lat) #"Latitude" goes on latitude
             ) +
  labs(
    title = "Corn Trial Site Locations (2014–2023)",
    x     = "Longitude",
    y     = "Latitude"
  )

```




```{r}

# Define Daymet bounding box (WGS-84)
min_lat <- 14.53
max_lat <- 52.00
min_lon <- -131.104
max_lon <- -52.95

# Filter merged to Daymet’s valid range, dropping any NA coords
merged_daymet <- merged_clean %>%
  filter(
    !is.na(lat),
    !is.na(lon),
    lat  >= min_lat,
    lat  <= max_lat,
    lon  >= min_lon,
    lon  <= max_lon
  )

# Report how many rows remain (and were dropped)
message("Rows kept: ", nrow(merged_daymet), 
        " (dropped: ", nrow(merged_clean) - nrow(merged_daymet), ")")


```

```{r unique_site_years_with_coords}

# Extract unique combinations of year, site, and their coordinates
site_year_df <- merged_daymet %>%
  select(year, site, lon, lat) %>%  # include longitude and latitude
  distinct() %>%                     # one row per unique combination
  arrange(year, site)

# Inspect the result
site_year_df

```

The following code chunk will download the weather data for all unique combinations of year, site, and their coordinates in the "site_year_df" object.

```{r}

weather_daymet_all <- site_year_df %>% 
  mutate(weather = pmap(list(.y = year, 
                             .site = site, 
                             .lat = lat, 
                             .lon = lon), 
                        function(.y, .site, .lat, .lon) 
                          download_daymet( 
                            site = .site, 
                            lat = .lat, #specifying ".lat" placeholder for "lat = " argument
                            lon = .lon, #specifying ".lon" placeholder for "lon = " argument
                            start = .y, 
                            end = .y, 
                            simplify = T,
                            silent = T) %>% #end of " download_daymet()" function
                          rename(.year = year,
                                 .site = site) 
                        )) 


```


The following code chunk will unnest the weather column. 

```{r}

weather_daymet_unnest <- weather_daymet_all %>%
  select(year, site, weather) %>% 
  unnest(weather) %>% 
  pivot_wider(names_from = measurement, 
              values_from = value) %>% 
  janitor::clean_names() #

weather_daymet_unnest

```


# Merging the weather data retrieved from Daymet to the "merged_clean" data

```{r merge_weather_with_data}

# Join daily weather onto your cleaned trial data by year & site

daymet_all_unnest <- merged_clean %>%
  left_join(
    weather_daymet_unnest,
    by = c("year", "site")
  )

daymet_all_unnest

```


## Exporting the merged data


```{r}

#write_csv(daymet_all_unnest,
          #"../data/merged_fieldweatherdata.csv"
          )

```



# End of retreiving weather data from Daymet 

# Start of feature engineering

## Setup 

```{r}
#| message: false
#| warning: false

install.packages("ggridges")

library(ggridges)
library(tidyverse)

```



```{r}

#fieldweather <- read_csv("../data/merged_fieldweatherdata.csv")

#fieldweather

```


```{r}

fe_month <- fieldweather %>%
  # Selecting needed variables
  dplyr::select(year, site, hybrid, lon, lat, yday, 
                yield = adjusted_yield, 
                planting.date = planting_date, 
                harvest.data = harvest_date, 
                planting.doy = plant_doy, 
                harvest.doy = harvest_doy, 
                season.length = season_len, 
                soil.ph = soilpH, 
                soil.om.pct = om_pct, 
                soil.k.ppm = soilk_ppm, 
                soil.p.ppm = soilp_ppm, 
                dayl.s = dayl_s, #to rename variable name from "dayl_s" to dayl.s
                prcp.mm = prcp_mm_day, #to rename variable name to "prcp.mm"
                srad.wm2 = srad_w_m_2,#to rename variable name to "srad.wm2"
                tmax.c = tmax_deg_c, #to rename variable name to "tmax.c"
                tmin.c = tmin_deg_c,#to rename variable name to "tmin.c"
                vp.pa = vp_pa #to rename variable name to "vp.pa"
                ) %>%
  # Creating a date class variable  
  mutate(date_chr = paste0(year, "/", yday)) %>% 
  mutate(date = as.Date(date_chr, "%Y/%j")) %>% 
  # Extracting month from date  
  mutate(month = month(date)) %>% 
  mutate(month_abb = month(date, label = T)) #To get abbreviated month name e.g., Jan, Feb, Mar,...,Dec #month_abb is in "ord" (ordinal) format

fe_month
```


Now, let's summarize daily weather variables based on month.  

```{r fe_month_sum}

fe_month_sum <- fe_month %>%
  group_by(year, site, hybrid, month_abb, yield) %>% #If we do a summarise() after group_by(), any column that's not in the group_by() is gone, so we need to include "strength_gtex" in the group_by() to include it in the data frame because "strength_gtex" is our response variable so we must keep it in the data frame 
  #Because we are gonna be applying a "summarize()" function to different columns, we are gonna use a function called "across()" 
  summarise(across(.cols = c(soil.ph, 
                             soil.om.pct, 
                             soil.k.ppm, 
                             soil.p.ppm,
                             dayl.s,
                             srad.wm2,
                             tmax.c,
                             tmin.c,
                             vp.pa),
                   .fns = mean, #do not indicate the actual function "mean()", just use the word "mean"
                   .names = "mean_{.col}"), #specifying the weather variables that we want their mean as new column variables #1st across() is applying the "mean" function (to summarize "mean")
            across(.cols = prcp.mm,
                   .fns = sum,
                   .names = "sum_{.col}"
                   ) #specifying the weather variable (prcp.mm) that we want its sum as new column variable #2nd across() is summarizing sum #2nd across() is applying the "sum" function (to summarize "sum")
            ) %>%
  ungroup() #To convert from "group" to "tibble"


fe_month_sum

```


Let's check tmax.c and prcp.mm for the first site-year and month. 

[Note: the following code chunk is for double checking to make sure that we did everything okay the way we intended] 

```{r}

fe_month %>%
  filter(year == 2014 & 
           site == "DEH1" &
           hybrid == "B37/MO17" &
           month_abb == "Jan") %>%
  summarise(tmax.c = mean(tmax.c),
            prcp.mm = sum(prcp.mm))


```

The code below will put the month as part of the column name. 

```{r fe_month_sum_wide}

fe_month_sum_wide <- fe_month_sum %>%
  pivot_longer(cols = mean_soil.ph:sum_prcp.mm) %>% 
  mutate(varname = paste0(name, "_", month_abb)) %>% 
  dplyr::select(-name, -month_abb) %>% 
  pivot_wider(names_from = varname,
              values_from = value) %>%
  # Rounding to one decimal point
  mutate(across(5:ncol(.), ~round(., 1) )) %>%
  select(-mean_soil.ph_NA, -mean_soil.om.pct_NA, -mean_soil.k.ppm_NA, -mean_soil.p.ppm_NA, -mean_dayl.s_NA, -mean_srad.wm2_NA, -mean_tmax.c_NA, -mean_tmin.c_NA, -mean_vp.pa_NA, -sum_prcp.mm_NA)


fe_month_sum_wide  

```


# EDA round 2  
Let's make a ridge plot to visualize the distribution of one variable over months.  

```{r, message=FALSE, warning=FALSE}

#install.packages("ggridges")
library(ggridges) #really powerful package to plot distributions e.g., density plot

ggplot(data = fe_month_sum,
       aes(x = mean_tmax.c,
           y = month_abb,
           fill = stat(x) #fill = stat(x): specific to "ggridges"
           )
       ) +
  geom_density_ridges_gradient(scale = 3,
                               rel_min_height = 0.01) + #"the 2nd argument is"rel_min_height = 0.01": to cut down the tails of the distribution to look nice
  scale_fill_viridis_c(option = "C") + #"viridis"scale_fill_viridis_c()": color-blind-friendly color scale #argument (option = "C") to change the color to "magma" because magma is weather related; "option = " varies from A to F
  theme(legend.position = "none") #"theme(legend.position = "none")": to remove the legend from the graph

```

Now let's do that for all weather variables [ we will automate using map2() to conduct iteration ]

```{r}

finalplots <- fe_month_sum %>%
  pivot_longer(cols = mean_soil.ph:sum_prcp.mm) %>% #we need to do pivot_longer() to iterate over the weather variables
  group_by(name) %>%
  nest() %>% #for iteration, we use "group_by()" followed by "nest()" in combo
  #we will use map2() since we need to iterate over 2 columns
  mutate(plot = map2(data, name, #map2() takes 2 arguments: the 1st argument becomes .x, the 2nd argument becomes .y #we must use map2() with a mutate() at first for iteration #"map2(data, name" : we want to iterate over "name" for "data"
                     ~ ggplot( data = .x, # .x represent "data" in the map2() function, so we need to use data = .x [= "data" from map2() ] to feed the "data" from map2() function as the data of ggplot() #.x is the iterating column of map2() that is a place holder for the 1st argument of map2()
       aes(x = value,
           y = month_abb,
           fill = stat(x)
           )
       ) +
  geom_density_ridges_gradient(scale = 3,
                               rel_min_height = 0.01) + 
  scale_fill_viridis_c(option = "C") + 
  theme(legend.position = "none") +
  labs(x = .y) #to rename the x-axis as the variable name # .y is the placeholder for "name" (i.e., variable name) in map2() function
                     )) 
  
finalplots

```




```{r}
#| message: false

finalplots$plot #$plot to print all the ggplots for each variables

```

#Export weather_monthsum.csv

```{r}

#write_csv(fe_month_sum_wide,
          # "../data/weather_monthsum.csv")

```



# Feature engineering "planting stages" and "harvest stages":

The code chunk below is to feature engineer planting and harvest stages based on the following cut-offs for planting day of the year (plant_doy) and harvest day of the year (harvest_doy):

Planting stages:
 - early: plant_doy ≤ 120
 - normal: plant_doy = 121–150
 - late: plant_doy > 150

Harvest stages:
 - early: harvest_doy ≤ 275
 - normal: harvest_doy = 276–305
 - late: harvest_doy > 305


```{r}

trait_avg_df <- trait_avg_df %>%
mutate(
    planting_stages = case_when(
      plant_doy <= 120                     ~ "early",
      plant_doy >= 121 & plant_doy <= 150  ~ "normal",
      plant_doy > 150                      ~ "late"
    ) %>% 
      fct_relevel("early","normal","late"),
    
    harvest_stages = case_when(
      harvest_doy <= 275                    ~ "early",
      harvest_doy >= 276 & harvest_doy <= 305 ~ "normal",
      harvest_doy > 305                     ~ "late"
    ) %>% 
      fct_relevel("early","normal","late")
  ) %>%
  select( -plant_doy, -harvest_doy) %>%
  ungroup()

trait_avg_df

View(trait_avg_df)

```
